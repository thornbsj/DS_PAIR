{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c208ccd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\lib\\site-packages\\transformers\\utils\\hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from datasets import Dataset,load_dataset\n",
    "from transformers import AutoTokenizer,AutoModelForSeq2SeqLM,DataCollatorForSeq2Seq,TrainingArguments,Seq2SeqTrainer,Seq2SeqTrainingArguments,EarlyStoppingCallback\n",
    "import pandas as pd\n",
    "import torch\n",
    "from rouge_chinese import Rouge\n",
    "import numpy as np\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b41ac452",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Langboat/mengzi-t5-base\"\n",
    "tokenizer_name = \"Langboat/mengzi-t5-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82e53883",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e86ba9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name,value in model.named_parameters():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c72e9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import LoraConfig, TaskType, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f709bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora_config = LoraConfig(\n",
    "#         r=8,\n",
    "#         target_modules=\".*\\.2[23].*query_key_value\",\n",
    "#         lora_dropout=0.05,\n",
    "#         task_type=TaskType.CAUSAL_LM\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06e99bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_peft_model(model,lora_config)\n",
    "# model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c17ca5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_excel(\"name_remark_description.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95d04005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3165790532514ce48680bc12b7d2d523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/969 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(dataset)\n",
    "def preprocess_func(example):\n",
    "    inputs = tokenizer(\"\\t\".join([\n",
    "        # \"物品类型：\"+example[\"type\"]\n",
    "        # ,\"子类型：\"+example[\"sub_type\"],\n",
    "        example[\"name\"]\n",
    "        ,example[\"remark\"]\n",
    "    ]),max_length=50,truncation=True)\n",
    "    outputs = tokenizer(text_target=example[\"description\"],max_length=100,truncation=True)\n",
    "    inputs[\"labels\"]=outputs[\"input_ids\"]\n",
    "    \n",
    "    return inputs\n",
    "dataset = dataset.map(preprocess_func,remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc6c54af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"test_seq2seq\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=32,\n",
    "    gradient_checkpointing=True,\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=1,\n",
    "    save_total_limit=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b64e049",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(model=model,args=train_args,train_dataset=dataset,\n",
    "                data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer,padding=True),\n",
    "                tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a01e03f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dfaaa4564ad4988a63efb80c75f721f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 13.5978, 'learning_rate': 4.9444444444444446e-05, 'epoch': 0.03}\n",
      "{'loss': 8.8957, 'learning_rate': 4.888888888888889e-05, 'epoch': 0.07}\n",
      "{'loss': 7.0244, 'learning_rate': 4.8333333333333334e-05, 'epoch': 0.1}\n",
      "{'loss': 6.4762, 'learning_rate': 4.7777777777777784e-05, 'epoch': 0.13}\n",
      "{'loss': 6.221, 'learning_rate': 4.722222222222222e-05, 'epoch': 0.17}\n",
      "{'loss': 6.0102, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.2}\n",
      "{'loss': 5.7734, 'learning_rate': 4.6111111111111115e-05, 'epoch': 0.23}\n",
      "{'loss': 5.7213, 'learning_rate': 4.555555555555556e-05, 'epoch': 0.26}\n",
      "{'loss': 5.4071, 'learning_rate': 4.5e-05, 'epoch': 0.3}\n",
      "{'loss': 5.3463, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}\n",
      "{'loss': 5.215, 'learning_rate': 4.388888888888889e-05, 'epoch': 0.36}\n",
      "{'loss': 5.1312, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}\n",
      "{'loss': 5.1914, 'learning_rate': 4.277777777777778e-05, 'epoch': 0.43}\n",
      "{'loss': 5.0871, 'learning_rate': 4.222222222222222e-05, 'epoch': 0.46}\n",
      "{'loss': 5.0295, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}\n",
      "{'loss': 5.035, 'learning_rate': 4.111111111111111e-05, 'epoch': 0.53}\n",
      "{'loss': 4.7087, 'learning_rate': 4.055555555555556e-05, 'epoch': 0.56}\n",
      "{'loss': 4.8668, 'learning_rate': 4e-05, 'epoch': 0.59}\n",
      "{'loss': 4.814, 'learning_rate': 3.944444444444445e-05, 'epoch': 0.63}\n",
      "{'loss': 4.8881, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.66}\n",
      "{'loss': 4.5205, 'learning_rate': 3.8333333333333334e-05, 'epoch': 0.69}\n",
      "{'loss': 4.6926, 'learning_rate': 3.777777777777778e-05, 'epoch': 0.73}\n",
      "{'loss': 4.4806, 'learning_rate': 3.722222222222222e-05, 'epoch': 0.76}\n",
      "{'loss': 4.4797, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.79}\n",
      "{'loss': 4.3794, 'learning_rate': 3.611111111111111e-05, 'epoch': 0.83}\n",
      "{'loss': 4.5121, 'learning_rate': 3.555555555555556e-05, 'epoch': 0.86}\n",
      "{'loss': 4.3767, 'learning_rate': 3.5e-05, 'epoch': 0.89}\n",
      "{'loss': 4.3967, 'learning_rate': 3.444444444444445e-05, 'epoch': 0.92}\n",
      "{'loss': 4.3332, 'learning_rate': 3.388888888888889e-05, 'epoch': 0.96}\n",
      "{'loss': 4.3796, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.99}\n",
      "{'loss': 4.2094, 'learning_rate': 3.277777777777778e-05, 'epoch': 1.02}\n",
      "{'loss': 4.2242, 'learning_rate': 3.222222222222223e-05, 'epoch': 1.06}\n",
      "{'loss': 4.2064, 'learning_rate': 3.1666666666666666e-05, 'epoch': 1.09}\n",
      "{'loss': 4.0937, 'learning_rate': 3.111111111111111e-05, 'epoch': 1.12}\n",
      "{'loss': 4.1672, 'learning_rate': 3.055555555555556e-05, 'epoch': 1.16}\n",
      "{'loss': 4.3138, 'learning_rate': 3e-05, 'epoch': 1.19}\n",
      "{'loss': 4.2306, 'learning_rate': 2.9444444444444448e-05, 'epoch': 1.22}\n",
      "{'loss': 4.4233, 'learning_rate': 2.8888888888888888e-05, 'epoch': 1.25}\n",
      "{'loss': 4.3179, 'learning_rate': 2.8333333333333335e-05, 'epoch': 1.29}\n",
      "{'loss': 4.2153, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.32}\n",
      "{'loss': 4.1029, 'learning_rate': 2.7222222222222223e-05, 'epoch': 1.35}\n",
      "{'loss': 4.1139, 'learning_rate': 2.6666666666666667e-05, 'epoch': 1.39}\n",
      "{'loss': 4.1055, 'learning_rate': 2.6111111111111114e-05, 'epoch': 1.42}\n",
      "{'loss': 3.8577, 'learning_rate': 2.5555555555555554e-05, 'epoch': 1.45}\n",
      "{'loss': 3.8565, 'learning_rate': 2.5e-05, 'epoch': 1.49}\n",
      "{'loss': 4.065, 'learning_rate': 2.4444444444444445e-05, 'epoch': 1.52}\n",
      "{'loss': 3.9906, 'learning_rate': 2.3888888888888892e-05, 'epoch': 1.55}\n",
      "{'loss': 4.0188, 'learning_rate': 2.3333333333333336e-05, 'epoch': 1.59}\n",
      "{'loss': 3.9479, 'learning_rate': 2.277777777777778e-05, 'epoch': 1.62}\n",
      "{'loss': 4.1478, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.65}\n",
      "{'loss': 3.9556, 'learning_rate': 2.1666666666666667e-05, 'epoch': 1.68}\n",
      "{'loss': 4.0621, 'learning_rate': 2.111111111111111e-05, 'epoch': 1.72}\n",
      "{'loss': 4.1256, 'learning_rate': 2.0555555555555555e-05, 'epoch': 1.75}\n",
      "{'loss': 3.9569, 'learning_rate': 2e-05, 'epoch': 1.78}\n",
      "{'loss': 3.9273, 'learning_rate': 1.9444444444444445e-05, 'epoch': 1.82}\n",
      "{'loss': 4.2148, 'learning_rate': 1.888888888888889e-05, 'epoch': 1.85}\n",
      "{'loss': 3.9824, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.88}\n",
      "{'loss': 4.0295, 'learning_rate': 1.777777777777778e-05, 'epoch': 1.92}\n",
      "{'loss': 3.8704, 'learning_rate': 1.7222222222222224e-05, 'epoch': 1.95}\n",
      "{'loss': 4.1606, 'learning_rate': 1.6666666666666667e-05, 'epoch': 1.98}\n",
      "{'loss': 3.6862, 'learning_rate': 1.6111111111111115e-05, 'epoch': 2.01}\n",
      "{'loss': 3.691, 'learning_rate': 1.5555555555555555e-05, 'epoch': 2.05}\n",
      "{'loss': 3.7778, 'learning_rate': 1.5e-05, 'epoch': 2.08}\n",
      "{'loss': 3.8079, 'learning_rate': 1.4444444444444444e-05, 'epoch': 2.11}\n",
      "{'loss': 3.7206, 'learning_rate': 1.388888888888889e-05, 'epoch': 2.15}\n",
      "{'loss': 4.0035, 'learning_rate': 1.3333333333333333e-05, 'epoch': 2.18}\n",
      "{'loss': 3.9557, 'learning_rate': 1.2777777777777777e-05, 'epoch': 2.21}\n",
      "{'loss': 3.9119, 'learning_rate': 1.2222222222222222e-05, 'epoch': 2.25}\n",
      "{'loss': 3.9115, 'learning_rate': 1.1666666666666668e-05, 'epoch': 2.28}\n",
      "{'loss': 3.9017, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.31}\n",
      "{'loss': 3.8195, 'learning_rate': 1.0555555555555555e-05, 'epoch': 2.34}\n",
      "{'loss': 3.8004, 'learning_rate': 1e-05, 'epoch': 2.38}\n",
      "{'loss': 3.6495, 'learning_rate': 9.444444444444445e-06, 'epoch': 2.41}\n",
      "{'loss': 3.7285, 'learning_rate': 8.88888888888889e-06, 'epoch': 2.44}\n",
      "{'loss': 3.8938, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.48}\n",
      "{'loss': 3.6863, 'learning_rate': 7.777777777777777e-06, 'epoch': 2.51}\n",
      "{'loss': 3.6316, 'learning_rate': 7.222222222222222e-06, 'epoch': 2.54}\n",
      "{'loss': 3.6538, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.58}\n",
      "{'loss': 3.7982, 'learning_rate': 6.111111111111111e-06, 'epoch': 2.61}\n",
      "{'loss': 3.8708, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.64}\n",
      "{'loss': 3.7362, 'learning_rate': 5e-06, 'epoch': 2.67}\n",
      "{'loss': 3.6574, 'learning_rate': 4.444444444444445e-06, 'epoch': 2.71}\n",
      "{'loss': 3.7884, 'learning_rate': 3.888888888888889e-06, 'epoch': 2.74}\n",
      "{'loss': 3.7696, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.77}\n",
      "{'loss': 3.7856, 'learning_rate': 2.777777777777778e-06, 'epoch': 2.81}\n",
      "{'loss': 4.0122, 'learning_rate': 2.2222222222222225e-06, 'epoch': 2.84}\n",
      "{'loss': 3.7227, 'learning_rate': 1.6666666666666667e-06, 'epoch': 2.87}\n",
      "{'loss': 3.9171, 'learning_rate': 1.1111111111111112e-06, 'epoch': 2.91}\n",
      "{'loss': 3.7491, 'learning_rate': 5.555555555555556e-07, 'epoch': 2.94}\n",
      "{'loss': 3.5647, 'learning_rate': 0.0, 'epoch': 2.97}\n",
      "{'train_runtime': 391.1072, 'train_samples_per_second': 7.433, 'train_steps_per_second': 0.23, 'train_loss': 4.460980206065708, 'epoch': 2.97}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=90, training_loss=4.460980206065708, metrics={'train_runtime': 391.1072, 'train_samples_per_second': 7.433, 'train_steps_per_second': 0.23, 'train_loss': 4.460980206065708, 'epoch': 2.97})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4fb4112",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()\n",
    "ipt = tokenizer(\n",
    "    #\"物品类型：{}\\n子类型：{}\\n\n",
    "    \"{}\\t{}\".format(\"钢铁庇佑戒指\",\"提升物理属性减伤率\").strip()\n",
    "    ,return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02045970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'圣职骑士的圣职, 可提升物理属性减伤率。 可提升物理属性减伤率。 据说是为骑士们所赠的戒指。'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(model.generate(**ipt,max_length=100)[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310550c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
